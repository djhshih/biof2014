---
title: "Markov chain Monte Carlo"
author: "David J. H. Shih"
format:
  beamer:
    include-in-header:
      - text: |
          \usepackage{amsfonts}
          \usepackage{amsmath}
          \usepackage{amssymb}
          \usepackage{amsthm}
          \usepackage{bm}
          \usepackage{bbm}
          \usepackage[english]{babel}
          \usepackage{fixmath}
          \usepackage{mathrsfs}
          \usepackage{mathtools}
          \usepackage{algorithm}
          \usepackage{algpseudocode}
classoption: t  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center");
```

## Intended learning outcomes {.c}

- Implement statistical models in Stan


## Notation

Many notations exist for specifying a statistical model.

For a discrete random variable $X$, the following notations are equivalent:
$$
\begin{aligned}
X &\sim g(\theta) \\
X &\sim g( \cdot \mid \theta) \\
X &\sim g(X \mid \theta) \\
X \mid \theta &\sim g(X \mid \theta) \\
P\left(X = x \mid \theta \right) &= g(x \mid \theta) \\
p\left(x \mid \theta \right) &= g(x \mid \theta) \\
p\left(x \mid \theta \right) &= g(x ; \theta) \\
f_X(x) &= g(x \mid \theta) \\
\end{aligned}
$$

## Notation

We will mostly use:
$$
\begin{aligned}
X &\sim g(\theta) \\
p\left(x \mid \theta \right) &= g(x \mid \theta)
\end{aligned}
$$

## Inverse transform sampling

We can sample $U \sim \text{Uniform}\left(0, 1\right)$ using a pseudorandom number generator.

We want to sample $X \sim F_X$.

Derive an invertible transform $T : [0, 1] \rightarrow \mathcal{R}$  
such that $T(U) \sim F_X$.

$$
\begin{aligned}
F_X(x) &= P\left(X \le x\right)     &\quad \text{(def'n of CDF)} \\
 &= P\left(T(U) \le x \right)       &\quad \text{(def'n of T)} \\
 &= P\left(U \le T^{-1}(x) \right)  &\quad \text{(T is invertible)} \\
 &= T^{-1}(x)                       &\quad (U \sim \text{Uniform})
\end{aligned}
$$

Therefore, $T(x) = F^{-1}_X(x)$ .


## Inverse transform sampling

Sample $X \sim F$.

\bigskip

\begin{algorithmic}
\Procedure{sample}{$F^{-1}$}
\State draw $u \sim \text{Uniform}\left(0, 1\right)$
\State \textbf{return} $F^{-1}(u)$
\EndProcedure
\end{algorithmic}


## Probabilistic acceptance

Accept $x$ with probabiliby $a$; otherwise, accept $x'$.

\bigskip

\begin{algorithmic}
\Procedure{accept}{$x, a, x'$}
\State draw $u \sim \text{Uniform}\left(0, 1\right)$
\If{$u \le a$}
 \State \textbf{return} $x$
\Else
 \State \textbf{return} $x'$
\EndIf
\EndProcedure
\end{algorithmic}


## Rejection sampling

We have proposal distribution $q$, and we want to sample from target distribution $p$.

The support of $q$ must contain the support of $p$.

Choose $M > 0$ s.t. $M q(x) \ge p(x) \; \forall x$.

Use $X \sim q$ to sample $Y \sim p$.

\bigskip

\begin{algorithmic}

\Procedure{rejection}{$q, p, M$}  \Comment proposal $q$, target $p$

\While{$y = \emptyset$}
 \State draw $x \sim q$
 \State $a \gets \frac{ p(x) }{ M q(x)}$
 \State $y$ \gets \Call{accept}{$x, a, \emptyset$}
\EndWhile

\State return $y$

\EndProcedure

\end{algorithmic}


## Markov chain Monte Carlo (MCMC)

MCMC is a class of algorithms for drawing samples $\symbfit{\theta}$ from a **target** distribution
$p(\symbfit{\theta} \mid x)$ using an unnormalized density s.t.
$$
p(\symbfit{\theta} \mid x) \propto \bar{p}(\theta \mid x) .
$$

\bigskip

\begin{algorithmic}
\Procedure{mcmc}{$q, \bar{p}, k, T$}  \Comment proposal $q$, target $\bar{p}$, cutoff $k$

\State initialize $\symbfit{\theta}^{(0)}$

\For{$t = 1 \ldots T $}
 \State draw $\symbfit{\theta} \sim q(\symbfit{\theta} \mid \symbfit{\theta}^{(t)})$
 \State compute acceptance probability $a$ using $\bar{p}(\symbfit{\theta})$
 \State $\symbfit{\theta}^{(t+1)}$ \gets 
  \Call{accept}{$\symbfit{\theta}, a, \symbfit{\theta}^{(t)}$}
\EndFor

\State \textbf{return} $\symbfit{\symbfit{\theta}}^{(k:T)}$

\EndProcedure

\end{algorithmic}

For sufficiently large $T$, $\symbfit{\symbfit{\theta}}^{(k:T)}$ 
will eventually converge to $p\left(\symbfit{\theta} \mid x \right)$.

---

\


## Metropolis-Hastings algorithm

$$
p_{\text{fwd}} = 
    \frac{ \bar{p}\left(\symbfit{\theta} \mid x\right) }
         { q\left(\symbfit{\theta} \mid \symbfit{\theta}^{({t})}\right) }
$$
$$
p_{\text{rev}} = 
    \frac{ \bar{p}\left(\symbfit{\theta}^{(t)} \mid x\right) }
         { q\left(\symbfit{\theta}^{({t})} \mid \symbfit{\theta}\right) }
$$

\bigskip
$$
\begin{aligned}
a &= \min 
 \left( 1, \frac{ p_{\text{fwd}} }{ p_{\text{rev}} } \right)
\\
 &= \min 
 \left( 1, 
  \frac{
   \bar{p}\left(\symbfit{\theta} \mid x\right) \; 
    q\left(\symbfit{\theta}^{({t})} \mid \symbfit{\theta}\right) 
  }
  {
   \bar{p}\left(\symbfit{\theta}^{(t)} \mid x\right) \; 
   q\left(\symbfit{\theta} \mid \symbfit{\theta}^{({t})}\right)
  }
 \right)
\end{aligned}
$$

---

\


## Gibbs sampling

Gibbs sampler always accepts ($a = 1$).

Define proposal $q$ based on the full conditional distributions.

For parameters $(\theta, \phi)$, we want to
sample from $p\left(\theta, \phi \mid x\right)$.

\bigskip

\begin{algorithmic}
\Procedure{gibbs}{$p, k, T$}  \Comment full conditional $p$, cutoff $k$

\State initialize $(\theta^{(0)}, \phi^{(0)})$

\For{$t = 1 \ldots T $}
 \State draw $\theta \sim  p\left( \theta \mid x, \phi \right)$
 \State $\theta^{(t+1)} \gets \theta$
 \State draw $\phi   \sim  p\left( \phi \mid x, \theta \right)$
 \State $\phi^{(t+1)} \gets \phi$
\EndFor

\State \textbf{return} $(\symbfit{\theta}^{(k:T)}, \symbfit{\phi}^{(k:T)})$

\EndProcedure

\end{algorithmic}


## Hamiltonian Monte Carlo

A variant of MCMC that uses Hamiltonian dynamics.

Used in the `stan` sampler.

Betancourt 2017. A conceptual introduction to Hamiltonian Monte Carlo.
https://arxiv.org/abs/1701.02434


## Summary

MCMC converges almost surely, as long as you are willing to wait indefinitely.

\bigskip


### Intended learning outcomes {.c}

- Implement statistical models in Stan
