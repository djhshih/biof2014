---
title: "Bernoulli and binomial distributions"
author: "David J. H. Shih"
format:
  beamer:
    include-in-header:
      - text: |
          \usepackage{amsfonts}
          \usepackage{amsmath}
          \usepackage{amssymb}
          \usepackage{amsthm}
          \usepackage{bm}
          \usepackage{bbm}
          \usepackage[english]{babel}
          \usepackage{fixmath}
          \usepackage{mathrsfs}
          \usepackage{mathtools}
classoption: t  
---

## Intended learning outcomes {.c}

- Recognize and apply Bernoulli and binomial distributions in statistical models
- Derive the binomial and negative binomial distributions


## Bernoulli distribution

### Definition

A random variable $X$ with domain $\mathcal{X} = \{0, 1\}$ has a Bernoulli distribution if
$$
\begin{aligned}
P \left( X = 1 \mid \theta \right) &\; = \; \theta \\
P \left( X = 0 \mid \theta \right) &\; = \; 1 - \theta
\end{aligned}
$$
where $\theta \in [0, 1]$ is the probability of success.

\bigskip

A more compact representation is
$$
P \left( X = x \mid \theta \right) \; = \;
 \theta^x \, (1 - \theta)^{1 - x}, \quad x \in \{0, 1\}
$$

We can also write
$$
\begin{aligned}
p \left( X = x \mid \theta \right) &= \text{Bernoulli}(x \mid \theta) \\
X &\sim \text{Bernoulli}(\theta)
\end{aligned}
$$


## Bernoulli trials

Suppose we have $N$ iid trials with success probability $\theta$,
resulting in random variables $X_1, \ldots, X_N$ such that
$$
X_i \sim \text{Bernoulli}(\theta) .
$$
This sequence of iid trials are called *Bernoulli* trials.

Define $\mathbold{X} = [X_1, \ldots, X_N]$.

$$
\begin{aligned}
P_{\mathbold{X}}\left( \mathbold{x} \right) 
 &= \prod_{i=1}^N P_{X_i}\left(x_i\right)
 \quad (\text{independent}) \\
 &= \prod_{i=1}^N \theta^{x_i} (1- \theta)^{1 - x_i} \\
 &= \theta^{\sum_i x_i} (1- \theta)^{1 - \sum_i x_i}
\end{aligned}
$$

---

Define $Y = \sum_{i=1}^N X_i$. Then, for $y \in \{0, 1, \ldots, N\}$,
$$
\begin{aligned}
P_Y \left( y \right) 
 &= P \left( \left\{ \mathbold{x} : \sum_i x_i = y \right\} \right) 
 &= \binom{N}{y} P_{\mathbold{X}}\left( \mathbold{x} \right) \\
 &= \binom{N}{y} \theta^y \, (1 - \theta)^{N - y} .
\end{aligned}
$$

Recall from previously,

| $y$ | $E = \{ \mathbold{x} \in \{0, 1\}^3 : \sum_i x_i = y \}$ |
|-----|----------------------------------------------------------|
|  0  | $\{ 000 \}$                                              |
|  1  | $\{ 100, 010, 001 \}$                                    |
|  2  | $\{ 110, 101, 011 \}$                                    |
|  3  | $\{ 111 \}$                                              |

: {tbl-colwidths="[10,60]"}

We have thus derived the **binomial distribution**.


## Binomial distribution

### Definition

A random variable $X$ has a binomial distribution if
$$
\begin{aligned}
P\left( X = x \mid N, \theta \right) \; = \;
\binom{N}{x} \theta^x \, (1 - \theta)^{N - x}, \quad x \in \{0, 1, \ldots, N\}
\end{aligned}
$$
where $\theta \in [0, 1]$ is the probability of success.


## Binomial theorem

### Theorem

For any real numbers $x$ and $y$, and integer $N \ge 0$,
$$
(x + y)^N = \sum_{i=0}^N \binom{N}{i} x^i y^{N - i} .
$$

### Proof

Base case $N = 0$.

LHS: $(x + y)^0 = 1$.

RHS: $\binom{0}{0} x^0 y^{0 - 0} = 1$.

\bigskip

Next, we will assume that the equation holds for $N - 1$, and 
prove that it also holds for $N$ (a.k.a. inductive step).

---

\bigskip

Assume $(x + y)^{N-1} = \sum_{i=0}^{N-1} \binom{N-1}{i} x^i y^{N - 1 - i}$. Then,
$$
\begin{aligned}
(x + y)^N &= (x + y) (x + y)^{N-1} \\
 &= (x + y) \sum_{i=0}^{N-1} \binom{N-1}{i} x^i y^{N - 1 - i} \\
 &= \left( \sum_{i=0}^{N-1} \binom{N-1}{i} x^{i+1} y^{N - 1 - i} \right)
  + \left( \sum_{i=0}^{N-1} \binom{N-1}{i} x^i y^{N - i} \right) .
\end{aligned}
$$

In the first sum, re-index with $j = i + 1$. In the second sum, substitute $j = i$.
This gives
$$
\begin{aligned}
(x + y)^N &=
    \left( \sum_{j=1}^{N} \binom{N-1}{j-1} x^j y^{N - j} \right)
  + \left( \sum_{j=0}^{N-1} \binom{N-1}{j} x^j y^{N - j} \right)
\end{aligned}
$$

---

\bigskip

Next, separate out the last term from the first sum, and the first term
from the second sum.
$$
\begin{aligned}
(x + y)^N &=
    \left( \sum_{j=1}^{N-1} \binom{N-1}{j-1} x^j y^{N - j} \right)
  + \left( \sum_{j=1}^{N-1} \binom{N-1}{j} x^j y^{N - j} \right) \\
  &\quad + \binom{N-1}{N-1} x^N y^{N-N}
  + \binom{N}{0} x^0 y^N \\
\end{aligned}
$$

---

\bigskip

By the binomial coefficient identity,
$$
\begin{aligned}
(x + y)^N &=
  \left( \sum_{j=1}^{N-1} \binom{N}{j} x^j y^{N - j} \right) 
  + \binom{N}{N} x^N y^{N-N}
  + \binom{N}{0} x^0 y^N \\
  &= \sum_{j=0}^{N} \binom{N}{j} x^j y^{N-j}
\end{aligned}
$$

Therefore, by induction, for $N = 0, 1, \ldots$,
$$
(x + y)^N = \sum_{j=0}^{N} \binom{N}{j} x^j y^{N-j} . \quad \blacksquare
$$


## Proof: Binomial$(N, \theta)$ is a probability distribution

For $x \in \{0, 1, \ldots, N\}$,
$$
\text{Binomial}(x \mid N, \theta) 
 = \binom{N}{x} \theta^x (1 - \theta)^{N - x} .
$$

### Non-negativity

$\binom{N}{x} \ge 0$ and $\theta \ge 0$.
$1 - \theta \ge 0$ since $\theta \le 1$.

Therefore, $\text{Binomial}(x \mid N, \theta) \ge 0 \quad \forall x$.

### Unit measure

For $x = \theta$ and $y = 1 - \theta$, by the binomial theorem,
$$
\sum_{x=0}^{N} \binom{N}{x} \theta^x (1 - \theta)^{N-x}
= (\theta + (1 - \theta))^N = 1.
$$


## Negative binomial distribution

The binomial distribution arises from a sequence of Bernoulli trials
where the number of trials $N$ is fixed.

A different distribution can also arise from a sequence of Bernoulli trials
if we pre-specify and fix the number of successes $R$ that we wish to
observe before we stop.

A random variable $X$ has a negative binomial distribution if
$$
P\left( X = x \mid R, \theta \right) \; = \;
 \binom{x - 1}{R - 1} \theta^R (1 - \theta)^{x - R},
 \quad x \in \{R, R + 1, \ldots\} ,
$$
where $X$ represents the total number of trials required to achieve a fixed number of successes, $R > 0$.


## Illustration



## Derivation of the negative binomial distribution

We have $X$ iid trials of success probability $\theta$ that contains
$R$ successes. $X$ is a random integer variable, whereas $R > 0$ is a 
fixed integer, and $X \le R$.

Denote the outcome of each trial by $Z_i$, so $R = \sum_{i=1}^X Z_i$.

Let $\mathbold{Z} = [Z_1, Z_2, \ldots, Z_X]$ represent the sequence of
outcomes. Similarly as before,
$$
P_\mathbold{Z} \left( \mathbold{z} \right) 
 = \prod_{i = 1}^{X} \theta^{z_i} (1 - \theta)^{1 - z_i}
 = \theta^{\sum_i z_i} (1 - \theta)^{X - \sum_i z_i}
 = \theta^R (1 - \theta)^{X - R}
$$

---

\bigskip

Denote the number of times that each event $\{X = x\}$ occurs as
$b(x, R)$, we have
$$
P_X(x) = P\left( \{\mathbold{z} : \sum_i^x z_i = R \} \right)
 = b(x, R) \, P_\mathbold{Z} \left( \mathbold{z} \right)  .
$$

For example, if $R = 2$,

| $x$ | $E = \{ \mathbold{z} \in \{0, 1\}^x : \sum_i z_i = R \}$ |
|-----|----------------------------------------------------------|
|  2  | $\{ 11 \}$                                               |
|  3  | $\{ 011, 101 \}$                                         |
|  4  | $\{ 0011, 0101, 1001 \}$                                 |
|  5  | $\{ 00011, 00101, 01001, 10001 \}$                       |
| $\ldots$ | $\ldots$                                            |

: {tbl-colwidths="[10,60]"}

---

\bigskip

Now, we need to find $b(x, R)$. We know that the $x$th trial must be a
success because we stop when we reach $R$ successes.
At trial $x - 1$, we have $R - 1$ successes, and there
are $\binom{x - 1}{R - 1}$ combinations.
Therefore, $b(x, R) = \binom{x - 1}{R - 1}$, and
$$
P_X\left(x \mid R, \theta \right) \; = \;
\binom{x - 1}{R - 1} \theta^R (1 - \theta)^{x - R} . \quad \blacksquare
$$


## Geometric distribution

A geometric distribution special case of the negative binomial
distribution where $R = 1$.

A random variable $X$ has a geometric distribution if
$$
P\left( X = x \mid \theta \right) \; = \;
 \theta (1 - \theta)^{x - 1},
 \quad x \in \{1, 2, \ldots\} ,
$$
where $\theta$ is the probability of success, and $X$ represents the
total number trials required for one success.


## Summary

"A journey of a thousand miles begins with a single step."  
- Lao Tzu

\bigskip

Casella & Berger 2002, sections 3.2, pages 86-91, 95-98.

\bigskip

### Intended learning outcomes {.c}

- Recognize and apply Bernoulli and binomial distributions in statistical models
- Derive the binomial and negative binomial distributions

