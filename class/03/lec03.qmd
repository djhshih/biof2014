---
title: "Uniform distributions"
author: "David J. H. Shih"
format:
  beamer:
    include-in-header:
      - text: |
          \usepackage{amsfonts}
          \usepackage{amsmath}
          \usepackage{amssymb}
          \usepackage{amsthm}
          \usepackage{bm}
          \usepackage{bbm}
          \usepackage[english]{babel}
          \usepackage{fixmath}
          \usepackage{mathrsfs}
          \usepackage{mathtools}
classoption: t  
---

## Intended learning outcomes {.c}

- Recognize and apply uniform distributions in statistical models
- Prove that Kolmogorov's axioms hold (or not) for given functions
- Derive expected values of random variables
- Apply empirical probability mass functions in models


## Discrete vs. continuous random variables

### Definitions

A random variable $X$ is *continuous* if its cdf $F_X(x)$ is a continuous
function of $x$.

A random variable $X$ is *discrete* if $F_X(x)$ is a step funtion of $x$.


## Discrete uniform distribution

### Definition

A random variable $X$ has a discrete uniform distribution if
$$
P \left( X = x \mid N \right) \; = \; \frac{1}{N}, \quad 
 x = 1, 2, \ldots, N,
$$
where $N > 0$ is an integer parameter.

Useful as an expression of ignorance.

Other possible notations are
$$
\begin{aligned}
p\left(X = x \mid N\right) &= \text{Uniform}\left( N\right) \\
X &\sim \text{Uniform}\left(N\right) \\
\end{aligned}
$$



## Examples

\begin{itemize}
\setlength\itemsep{2em}
\item outcome of a fair coin flip
\item outcome of a fair dice roll
\item number on a drawn card from a well-shuffled deck
\item biological sex of humans
\end{itemize}


## Kolmogorov's axioms of probability

Given a sample space $\mathcal{S}$, a probability function $P$
satisfies
$$
\begin{aligned}
1. \; & P\left( \mathcal{A} \right) \ge 0 \quad \forall \mathcal{A} \subseteq S. 
      \quad & (\text{non-negativity})   
      \\
2. \; & P\left( \mathcal{S} \right) = 1 . 
      \quad & (\text{unit measure}) \\
3. \; & \text{For any } \mathcal{A}_1, \mathcal{A}_2, \ldots \subseteq S \\
      & \text{ s.t. } \mathcal{A}_i \cap \mathcal{A}_j = \emptyset \; \forall i \neq j, \\
      & P \left( \bigcup_{i = 1}^\infty \mathcal{A}_i \right) 
      \; = \; \sum_{i = 1}^\infty P\left(\mathcal{A}_i\right) .
      \quad & (\text{additivity}) \\
\end{aligned}
$$

Remark: Given the sample space $\mathcal{X}$ of the random variable $X$,
the induced probability function $P_X$ must be defined properly so that
it satisifies Kolmogorov's axioms.


## Proof: $\text{Uniform}(N)$ is a probability distribution

Prove that $P_X(x) = \text{Uniform}(N)$ satisifies Kolmogorov's axioms 
of probability.

### Non-negativity

For any $\mathcal{A} \subseteq \mathcal{X}$ where $A = \{ a_1, \ldots a_K \}$,
$P_X(\mathcal{A}) = \sum_k P_X(a_k)$ by the additivity axiom.

Further, $\sum_k P_X(a_k) = \sum_{k=1}^K \frac{1}{N} = \frac{K}{N}$.

Finally, $\frac{K}{N} > 0$ since $N > 0$ and $K > 0$.

Therefore, $P_X(\mathcal{A}) \ge 0$ for any $\mathcal{A} \subseteq \mathcal{X}$.


## Proof: $\text{Uniform}(N)$ is a probability distribution

### Unit measure

Given $\mathcal{X} = \{x_1, \ldots, x_N\}$,
$$
\begin{aligned}
P_X\left( \mathcal{X} \right) 
 &= P_X\left( \{s_1, \ldots, s_N\} \right) \\
 &= \sum_{i=1}^N P(s_i) \quad (\text{additivity}) \\
 &= \sum_{i=1}^N \frac{1}{N} = \frac{N}{N} = 1
\end{aligned}
$$


## Proof: $\text{Uniform}(N)$ is a probability distribution

Recall that, for a discrete random variable $X$,
$$
P_X(x) = P\left( \{ s \in \mathcal{S} : X(s) = x \} \right).
$$

It is a given that $P(s)$ is a probability function.


### Additivity

Consider any $\mathcal{A}, \mathcal{B} \subseteq \mathcal{X}$ s.t. 
$\mathcal{A} \cap \mathcal{B} = \emptyset$.

Note that $\mathcal{A} = {a_1, \ldots, a_K}$ and $\mathcal{B} = {b_1, \ldots, b_L}$.

Define $c_1 = a_1, \ldots, c_K = a_K, c_{K+1} = b_1, \ldots, c_{K+L} = b_L$.

#### Proof outline

We will show that $P_X(\mathcal{A}) = \frac{K}{N}$, 
and similarly, $P_X(\mathcal{B}) = \frac{L}{N}$.

We will then show
$P_X(\mathcal{A} \cup \mathcal{B}) = \frac{K}{N} + \frac{L}{N}.$

Therefore, $P_X(\mathcal{A} \cup \mathcal{B}) = P_X(\mathcal{A}) + P_X(\mathcal{B})$.

Our derivation can be extended to any number of subsets
$\mathcal{A}, \mathcal{B}, \mathcal{C}, \ldots$ of $\mathcal{X}$, which
completes the proof.

---

$$
\begin{aligned}
P_X(\mathcal{A}) 
 &= P_X\left( \bigcup_{k=1}^K a_k \right) \\
 &= P\left( \bigcup_{k=1}^K \{ s_k \in \mathcal{S} : X(s_k) == a_k \} \right) 
  \quad (\text{definition of }P_X) \\
 &= \sum_{k=1}^K P\left( \{s_k \in \mathcal{S} : X(s_k) == a_k \} \right)
  \quad (\text{additivity of }P) \\
 &= \sum_{k=1}^K P_X( a_k )
  \quad (\text{definition of }P_X) \\
 &= \sum {k=1}^K \frac{1}{N}
  \quad (\text{definition of Discrete}(x \mid N)) \\
 &= \frac{K}{N}
\end{aligned}
$$

Similarly as above, we can also show $P_X(\mathcal{B}) = \frac{L}{N}$.

---

$$
\begin{aligned}
P_X \left( \mathcal{A} \cup \mathcal{B} \right)
 &= P_X \left( \left(\bigcup_{k=1}^K a_k\right) \cup \left(\bigcup_{l=1}^L b_k\right) \right)
 &= P\left(
   \bigcup_{k=1}^K \{s_k \in \mathcal{S} : X(s_k) == a_k \} \cup 
   \bigcup_{l=1}^L \{s_k \in \mathcal{S} : X(s_k) == b_l \}
  \right) 
  \quad (\text{definition of }P_X) \\
 &= P\left(
   \bigcup_{k=1}^{K+L} \{s_k \in \mathcal{S} : X(s_k) == c_k \}
  \right)
  \quad (\text{definition of }c_k) \\
 &= \sum_{k=1}^{K+L} P\left( \{s_k \in \mathcal{S} : X(s_k) == c_k \} \right)
  \quad (\text{additivity of }P) \\
 &= \sum_{k=1}^{K+L} P_X( c_k )
  \quad (\text{definition of }P_X) \\
 &= \sum_{k=1}^{K+L} \frac{1}{N}
  \quad (\text{definition of Discrete}(x \mid N)) \\
 &= \frac{K}{N} + \frac{L}{N} 
  = P_X(\mathcal{A}) + P_X(\mathcal{B}) .
\end{aligned}
$$

---

Therefore, for any $\mathcal{A}, \mathcal{B} \subseteq \mathcal{X}$
s.t. $\mathcal{A} \cap \mathcal{B} = \emptyset$,
$$
P_X \left( \mathcal{A} \cup \mathcal{B} \right) = 
 P_X \left(\mathcal{A}\right) + P_X \left(\mathcal{A}\right) .
$$

Using similar steps as above, we can show for any pairwise disjoint 
subsets $\mathcal{A}_1, \mathcal{A}_2, \ldots$ of $\mathcal{X}$ that
$$
P_X \left( \bigcup_{i=1}^\infty \mathcal{A} \right) = 
 \sum_{i=1}^\infty P_X \left(\mathcal{A_i}\right) .
 \quad \blacksquare
$$

Further, using almost the same derivation as above, given that
the probability function $P$ on sample space $\mathcal{S}$ satisfies
the addivity axiom, we can show that the previously defined $P_X(x)$ 
also satisfies the additivity axiom.


## Probability distribution

Since the additvity axiom holds for any $P_X(x)$, to show that 
a given $P_X(x)$ correspond to a *proper* probability distribution, 
we just need to show that $P_X(x)$ satisfies the first 
two Komomgorov's axioms. Specifically,
$$
\begin{aligned}
1.\; & P_X(x) \ge 0 \quad \forall x \in \mathcal{X} . & \\
\\
2.\; & \sum_{x \in \mathcal{X}} P_X(x) = 1  & \quad \text{if }X\text{ is discrete}; \\
     & \int_{\mathcal{X}} f_X(x) \, dx = 1  & \quad \text{if }X\text{ is continuous} .
\end{aligned}
$$


## Continuous uniform distribution

### Definition

A random variable has a continuous uniform distribution over an 
interval $[a, b]$ if 
$$
f_X(x \mid a, b) = 
\begin{cases}
\frac{1}{b - a} & \text{ if } x \in [a, b] \\
0 & \text{otherwise} ,
\end{cases}
$$
where $b > a$.

We can also write
$$
\begin{aligned}
p\left( X = x \mid a, b\right) &= \text{Uniform}\left(x \mid a, b\right) \\
X &\sim \text{Uniform}\left(a, b\right)
\end{aligned}
$$


## Proof: Uniform$(a, b)$ is probability distribution

Prove that $f_X(x) = \text{Uniform}(x \mid a, b)$ is a probability distribution

### Non-negativity

If $x \notin [a, b]$, $f_X(x) = 0$.

If $x \in [a, b]$, $f_X(a) = \frac{1}{b - a} \ge 0$ since $b > a$.

### Unit measure

$$
\begin{aligned}
\int_{-\infty}^{\infty} f_X(x) \, dx
 &= \int_{a}^{b} f_X(x) \, dx
  = \int_{a}^{b} \frac{1}{b - a} \, dx \\
 &= \left[ \frac{x}{b - a} \right]_{x = b}
  - \left[ \frac{x}{b - a} \right]_{x = a} \\
 &= \frac{b}{b - a} - \frac{a}{b - a}
  = 1
\end{aligned}
$$


## Continuous uniform distribution

If model parameters $a$ and $b$ are spread further apart (at the same rate)
to cover a larger interval $[a, b]$, we have
$$
f_X(x \mid -a, a) = \frac{1}{2a}, 
$$
where $b = -a$.

Since $\lim_{a \rightarrow \infty} f_X(x \mid -a, a) = 0$,
a uniform distribution spanning $(-\infty, \infty)$ is *not*
a proper probability distribution, due to violation of the 
unit measure axiom.

Instead, we sometimes define an improper uniform distribution by
$$
p_X(x) \propto 1 ,
$$
where is also not a proper distribution.


## Expectation

### Definition

The *expected value* or *mean* of a random variable $g(X)$ is
$$
\text{E} \left( g(X) \right) \triangleq
\begin{cases}
\int_{-\infty}^{\infty} g(x) f_X(x) dx
 \quad & \text{if }X\text{ is continuous} \\
\sum_{x \in \mathcal{X}} g(x) f_X(x)
 \quad & \text{if }X\text{ is discrete},
\end{cases}
$$
provided that the integral or sum exists.

If $\text{E} \left( g(X) \right) = \infty$,
we say that $\text{E} \left( g(X) \right)$ does not exist.


## Example

Given $X \sim \text{Uniform}(a, b)$, find $\text{E} \left(X^2\right)$.


## Variance

### Definition

The *variance* of a random variable $g(X)$ is
$$
\text{Var} \left( X \right) \triangleq
 \text{E} \left[ \left( X - \text{E}\left(X\right) \right)^2 \right] .
$$


### Alternative formula

Variance can be expressed as
$$
\text{Var} \left( X \right) =
 \text{E} \left( X^2 \right) - \left( \text{E}(X) \right)^2 .
$$
This formula is useful in derivations, but calculating variance 
with this formula will lead to numerical instability.


## Empirical probability mass function

Uniform distributions are convenient starting points if we have no prior
knowledge about the random variable.

We should always ask if a distribution in a model is appropriate for the
problem.

We can also estimate the pmf from the observed data.

Given *independent and identically distributed* (iid) samples $\{x_1, \ldots, x_N\}$,
an **empirical pmf** for random variable $X$ is given by
$$
\hat{f}_N(x) = \frac{1}{N} \sum_{i=1}^N I(x_i = x) ,
$$
where $I$ is the indicator function.


## Empirical cumulative distribution function

Given iid samples $\{x_1, \ldots, x_N\}$,
an **empirical cdf** for random variable $X$ is given by
$$
\hat{F}_N(x) = \frac{1}{N} \sum_{i=1}^N I(x_i \le x) .
$$

The Glivenko-Cantelli theorem states that $\hat{F}_N(x)$ converges
to $F(x)$. That is, as $N \rightarrow 0$, with probability 1,
$$
\sup_{x \in \mathcal{X}} \left| \hat{F}_N(x) - F(x) \right| = 0 ,
$$
where $\sup$ is the supremum.


## Summary

"Probability is a measure of our ignorance."  - Richard Jeffreys

\bigskip

Casella & Berger 2002, sections 3.2, pages 85-86, 98-99

\bigskip

### Intended learning outcomes {.c}

- Prove that Kolmogorov's axioms hold (or not) for given functions
- Recognize and apply uniform distributions in statistical models
- Derive expected values of random variables
- Apply empirical probability mass functions in models
