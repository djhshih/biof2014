---
title: "Model fitting and parameter estimation"
author: "David J. H. Shih"
format:
  beamer:
    include-in-header:
      - text: |
          \usepackage{amsfonts}
          \usepackage{amsmath}
          \usepackage{amssymb}
          \usepackage{amsthm}
          \usepackage{bm}
          \usepackage{bbm}
          \usepackage[english]{babel}
          \usepackage{fixmath}
          \usepackage{mathrsfs}
          \usepackage{mathtools}
          \DeclareMathOperator*{\argmax}{argmax}
classoption: t  
---

## Intended learning outcomes {.c}

- Fit models to data by estimating model parameters


## Model fitting

Use data $\symbfit{x}$ to estimate parameters $\symbfit{\theta}$ of the model.

### Maximum likelihoood estimation (MLE)
$$
\hat{\symbfit{\theta}} 
 = \argmax_{\symbfit{\theta}}
  p\left( \symbfit{x} \mid \symbfit{\theta} \right) .
$$

### Maximum *a posteriori* (MAP)
$$
\hat{\symbfit{\theta}} 
 = \argmax_{\symbfit{\theta}}
  p\left( \symbfit{\theta} \mid \symbfit{x} \right)
 = \argmax_{\symbfit{\theta}}
  p\left( \symbfit{x} \mid \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \right) .
$$

### Full Bayesian
$$
p\left( \symbfit{\theta} \mid \symbfit{x} \right)
 = \frac{
  p\left( \symbfit{x} \mid \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \right)
 }{
  p\left( \symbfit{x} \right)
 } .
$$


## Parameter estimates

### Point estimate

\vspace{3em}

### Posterior for discrete random variable

\vspace{5em}

### Posterior for continuous random variable

\vspace{5em}


## Generating new samples

Use $\hat{\symbfit{\theta}}$ or 
$p\left( \symbfit{\theta} \mid \symbfit{x} \right)$ to generate 
a new data point $\tilde{x}$.

### With point estimate from MLE or MAP
$$
\tilde{x} \sim 
 p\left( \tilde{x} \mid \hat{\symbfit{\theta}} \right) .
$$

### With posterior predictive distribution
$$
\begin{aligned}
p\left( \tilde{x} \mid \symbfit{x} \right)
 &= \int_{\symbfit{\Theta}}
  p\left( \tilde{x} \mid \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \mid \symbfit{x} \right) \,
 d \symbfit{\theta}  \\
\tilde{x}
 &\sim p\left( \tilde{x} \mid \symbfit{x} \right) .
\end{aligned}
$$

### With posterior distribution
$$
\symbfit{\theta}
 \sim p(\symbfit{\theta} \mid \symbfit{x}), \quad
\tilde{x}
 \sim p\left( \tilde{x} \mid \symbfit{\theta} \right) .
$$


## Fitting discriminative models

Use data $(\symbfit{x}, \symbfit{y})$ to estimate parameters 
$\symbfit{\theta}$ of the model and to predict new outcome
$\hat{y}$ given new data point $\tilde{x}$.

### MLE
$$
\hat{\symbfit{\theta}} 
 = \argmax_{\symbfit{\theta}}
  p\left( \symbfit{y} \mid \symbfit{x} , \symbfit{\theta} \right) .
$$

### MAP 
$$
\hat{\symbfit{\theta}} 
 = \argmax_{\symbfit{\theta}}
  p\left( \symbfit{\theta} \mid \symbfit{y}, \symbfit{x} \right)
 = \argmax_{\symbfit{\theta}}
  p\left( \symbfit{y} \mid \symbfit{x}, \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \right) .
$$

### Full Bayesian
$$
p\left( \symbfit{\theta} \mid \symbfit{y}, \symbfit{x} \right)
 = \frac{
  p\left( \symbfit{y} \mid \symbfit{x}, \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \right)
 }{
  p\left( \symbfit{y} \mid \symbfit{x} \right)
 } .
$$


## Predicting new outcomes

### With point estimate from MLE or MAP
$$
\hat{y}
 = \argmax_y p\left( y \mid \tilde{x}, \hat{\symbfit{\theta}} \right) .
$$

### With posterior predictive distribution
$$
\begin{aligned}
p\left( \tilde{y} \mid \tilde{x} \right)
 &= \int_{\symbfit{\Theta}}
 p\left( \tilde{y} \mid \tilde{x}, \symbfit{\theta} \right) \,
 p\left( \symbfit{\theta} \mid \symbfit{y}, \symbfit{x} \right)
 d \symbfit{\theta} ,
\\
\tilde{y} &\sim p\left( \tilde{y} \mid \tilde{x} \right) .
\end{aligned}
$$

### With posterior distribution
$$
\symbfit{\theta}
 \sim p\left(\symbfit{\theta} \mid \symbfit{y}, \symbfit{x}\right), \quad
\tilde{y}
 \sim p\left( \tilde{y} \mid \tilde{x}, \symbfit{\theta} \right) .
$$


## Full Bayesian

### Basic model
$$
p\left( \symbfit{\theta} \mid \symbfit{x} \right)
 = \frac{
  p\left( \symbfit{x} \mid \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \right)
 }{
  p\left( \symbfit{x} \right)
 } .
$$

### Discriminative model
$$
p\left( \symbfit{\theta} \mid \symbfit{y}, \symbfit{x} \right)
 = \frac{
  p\left( \symbfit{y} \mid \symbfit{x}, \symbfit{\theta} \right) \,
  p\left( \symbfit{\theta} \right)
 }{
  p\left( \symbfit{y} \mid \symbfit{x} \right)
 } .
$$

### Generative model
$$
p\left( \symbfit{\theta} \mid \symbfit{x}, \symbfit{y} \right)
 = \frac{
  p\left( \symbfit{x} \mid \symbfit{\theta}, \symbfit{y} \right) \,
  p\left( \symbfit{\theta} \mid \symbfit{y} \right)
 }{
  p\left( \symbfit{x} \mid \symbfit{y} \right)
 } .
$$


## Maximum likelihood

$$
L\left( \theta \right) = p\left( \symbfit{x} \mid \theta \right)
\quad
\text{or}
\quad
L\left( \theta \right) = p\left( \symbfit{y} \mid \symbfit{x}, \theta \right)
\quad
\text{or}
\quad
L\left( \theta \right) = p\left( \symbfit{x} \mid \symbfit{y}, \theta \right)
$$

$$
p\left( \symbfit{x} \mid \theta \right)
 = \prod_i p\left( x_i \mid \theta \right)
$$

$$
\ell(\theta) = \log L\left( \theta \right)
$$

$$
\log p\left( \symbfit{x} \mid \theta \right)
 = \sum_i \log p\left( x_i \mid \theta \right)
$$

$$
\frac{ d \ell(\theta) }{ d \theta } = 0
$$


## Maximum *a posteriori*

$$
\begin{aligned}
p\left( \theta \mid \symbfit{x} \right)
 &\propto p\left( \symbfit{x} \mid \theta \right) \, p\left( \theta \right)
\\ &\text{or} \\
p\left( \theta \mid \symbfit{y}, \symbfit{x} \right)
 &\propto p\left( \symbfit{y} \mid \symbfit{x}, \theta \right) \, p\left( \theta \right)
\quad
\\ &\text{or} \\
p\left( \theta \mid \symbfit{x}, \symbfit{y} \right)
 &\propto p\left( \symbfit{x} \mid \symbfit{y}, \theta \right) \, p\left( \theta \mid y \right)
\end{aligned}
$$

$$
\ell(\theta) = \log p\left( \theta \mid \symbfit{x} \right)
$$

$$
\log p\left( \symbfit{x} \mid \theta \right)
 = \sum_i \log p\left( x_i \mid \theta \right)
$$

$$
\frac{ d \ell(\theta) }{ d \theta } = 0
$$


## MLE vs. MAP vs. full Bayesian

MLE is an approximation of MAP with a uniform prior.

MAP is an approximation of the posterior.


## Summary

"All models are wrong. Some are useful."
- George Box

\bigskip

Blais 2014, chapter 6.


### Intended learning outcomes {.c}

- Fit models to data by estimating model parameters
