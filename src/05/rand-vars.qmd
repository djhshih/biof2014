---
title: "Random variables"
author: "David J. H. Shih"
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsfonts}
          \usepackage{amsmath}
          \usepackage{amssymb}
          \usepackage{amsthm}
          \usepackage{fixmath}
          \usepackage{mathrsfs}
          \usepackage{mathtools}
classoption: t  
---

## Intended learning outcomes {.c}

- Recognize and define events and random variables
- Implement code to sample values for a random variable


## Event

### Definition

Given a sample space $\mathcal{S} = \{s_1, \ldots, s_J\}$,
an event $\mathcal{E}$ is a subset of the sample space $\mathcal{S}$: 
$\mathcal{E} \subseteq \mathcal{S}$.

\bigskip

$\mathcal{E}$ occurs if any $e_j \in \mathcal{E}$ is observed.

Given a probability function $P$,
$$
\begin{aligned}
P\left(\mathcal{E} \right)
\; &= \; P\left( \bigcup_j^\infty e_j \right) \quad (\text{expansion}) \\
\; &= \; \sum_j^\infty P\left( e_j \right) \quad (\sigma\text{-additivity})
\end{aligned}
$$

Further, if $P\left(s_i\right) = P\left(s_j\right)$ 
for all $s_i, s_j \in \mathcal{S}$, then
$$
P\left(\mathcal{E}\right) = \frac{|\mathcal{E}|}{|\mathcal{S}|} ,
$$
where $|\mathcal{A}|$ is the number of elements in a set $\mathcal{A}$.


## Example

Sequence: Toss a coin 2 times independently.

Assume that the coin toss results in either head ($H$) or tail ($T$).

Define event $\mathcal{E}$ as the occurrence of one head:
$$
\mathcal{E} = \{ HT, TH \}
$$
Derive $P\left( \mathcal{E} \right)$.

### Answer

$$
\begin{aligned}
P\left( \mathcal{E} \right) 
&= P\left( HT \cup TH \right)
 &\quad (\text{expansion}) \\
&= P\left( HT \right) + P\left( TH \right)
 &\quad (\sigma\text{-additivity}) \\
&= P\left( H \right) P\left(T\right) + 
 P\left(T\right) P\left(H \right)
 &\quad (\text{independence}) \\
&= 2 \; P\left(H\right) P\left(T\right)
 \\
&= 2 \; P\left(H\right) P\left(H^c\right)
 &\quad (\text{assumption}) \\
&= 2 \; P\left(H\right) (1 - P\left(H\right))
 &\quad (\text{complement rule}) \\
\end{aligned}
$$


## Randomness

Apparent or actual lack of definite pattern in information.

Compatible with a deterministic or stochastic process.


### Examples

- flipping a coin
- drawing cards from a shuffled deck
- atmospheric noise ([random.org][1])
- somatic mutations

[1]: www.random.org


## Random variable

### Definition

Given a sample space $\mathcal{S} = \{s_1, \ldots, s_J\}$ and 
a probability function $P$,
a **random variable** $X$ is a function that maps from $\mathcal{S}$ onto 
real numbers with domain $\mathcal{X} = \{x_1, \ldots, x_N\}$.

### Induced probability function

Each realization $x_i \in \mathcal{X}$ corresponds to an event 
$\mathcal{E}_i \subseteq \mathcal{S}$
such that
$$
P_X \left( X = x_i \right) \; = \; P \left( \mathcal{E_i} \right),
$$
where $\mathcal{E}_i = \{ s_j \in \mathcal{S} : X(s_j) = x_i \}$ and 
$P_X$ is the *induced* probability function on $\mathcal{X}$.

\bigskip

English equivalent of the equation:

"Under the induced probability function $P_X$, the probability that the 
random variable $X$ equals a value $x_i$ is given by the probability 
of the event $\mathcal{E}_i$,
which is the set of all outcome $s_j$ in the sample space 
$\mathcal{S}$ such that random variable $X$ maps $s_j$ to $x_i$."

\bigskip

Remark: We often abbreviate $P_X\left(X = x_i\right)$ as 
$P_X\left(x_i\right)$ or $P\left(x_i\right)$.


### Example 1

Define random variable $X$ as tomorrow's weather.

| $x_i$ | $\mathcal{E}_i = \{ s_j \in S : X(s_j) = x_i \}$   |
|-------|----------------------------------------------------|
|       |                                                    |
|       |                                                    |
|       |                                                    |
|       |                                                    |
|       |                                                    |
|       |                                                    |

: {tbl-colwidths="[30,60]"}


### Example 2

Consider a sequence of 3 coin flips.

Define random variable $X$ as the number of heads.

| $x_i$ | $\mathcal{E}_i = \{ s_j \in S : X(s_j) = x_i \}$   |
|-------|----------------------------------------------------|
|       |                                                    |
|       |                                                    |
|       |                                                    |
|       |                                                    |
|       |                                                    |
|       |                                                    |

: {tbl-colwidths="[30,60]"}


## Uncountable sample space

So far, we assumed that sample space $\mathcal{S} = \{s_1, \ldots, s_J\}$,   
i.e. $\mathcal{S}$ is (countable and) finite.

We also assumed that $\mathcal{X} = \{x_1, \ldots, x_N\}$ is finite.

If $\mathcal{X}$ is uncountable, we can define the induced probability
function for some set $\mathcal{A} \subset \mathcal{X}$ as
$$
P_X \left( X \in \mathcal{A} \right)
\; = \; P \left( \{s \in \mathcal{S} : X(s) \in \mathcal{A} \} \right) .
$$

## Cumulative distribution function

### Definition

The cumulative distribution function (cdf) of a random variable $X$ is defined
by
$$
F_X(x) \triangleq P_X(X \le x), \quad \text{ for all } x.
$$

### Theorem

A function $F(x)$ is a cdf if and only if

$$
\begin{aligned}
&\text{a.} \; \lim_{x \rightarrow - \infty} \, F(x) = 0 \quad \text{and} \quad
   \lim_{x \rightarrow \infty} \, F(x) = 1 . \\
&\text{b.} \; F(x_1) \le F(x_2) \; \text{ for all } \; x_1 \le x_2  
\quad  (\text{non-decreasing}) . \\
&\text{c.} \; \lim_{x \rightarrow x_0^+} F(x) = F(x_0) \quad (\text{right-continuous}) .
\end{aligned}
$$

### Examples

#### Continuous function

\vspace{80pt}

#### Step function

\vspace{80pt}


## Probability mass function

### Definition

The probability mass function (pmf) of a discrete random variable $X$ is given
by
$$
f_X(x) \; \triangleq \; P_X\left( X = x \right) \quad \text{ for all } x.
$$

### Example

Sequence: Toss a fair coin 3 independent times.

Define random variable $X$ as the number of heads.

Calculate $f_X(x_i) = P_X\left(X = x_i\right)$ for all $x_i \in X$.

| $x_i$ | $\mathcal{E}_i$  | $f_X(x_i)$                      |
|-------|------------------|---------------------------------|
|       |                  |                                 |
|       |                  |                                 |
|       |                  |                                 |
|       |                  |                                 |
|       |                  |                                 |
|       |                  |                                 |
|       |                  |                                 |
|       |                  |                                 |

: {tbl-colwidths="[15,60,15]"}


## $f_X(x)$ for continuous random variable?

For a continuous random variable $X$, we might try to define
$$
f_X(x) \; = \; P_X\left( X = x \right) .
$$

However, $P_X(X = x) \; = \; 0$ for all $x \in \mathcal{X}$ here.

So, we need to define $f_X(x)$ differently for continuous random variables.


### Lemma

For a continuous random variable $X$, 
$P_X(X = x) \; = \; 0, \forall \, x \in \mathcal{X}$.

### Proof

Recall from set theory that
$$
\begin{aligned}
(1) &\quad A \subseteq B \; \Rightarrow \; P(A) \le P(B)  \\
(2) &\quad P(A \cap B) = P(B) - P(B \cap A^c)
\end{aligned}
$$

For any $\epsilon > 0$, we have
$$
\{r \in \mathcal{X} : r = x\} \subseteq \{r \in \mathcal{X} : x - \epsilon < r \le x \} .
$$
As short-hand, we will write
$$
\{ X = x \} \; \subseteq \; \{ x - \epsilon < X \le x \} .
$$

By (1), 
$$
P\left( X = x \right) \; \le \; P \left(x - \epsilon < X \le x \right) .
$$

By definition,
$$
\begin{aligned}
P \left(x - \epsilon < X \le x \right) 
 \; &= \; P \left( \{ r \in \mathcal{X}: x - \epsilon < r \}
  \, \cap \, \{ r \in \mathcal{X} : r \le x \} \right) \\
 &= P \left( \{ x - \epsilon < X \} \, \cap \, \{ X \le x \} \right)
\end{aligned}
$$

By (2),
$$
\begin{aligned}
&P \left( \{ x - \epsilon < X \} \, \cap \, \{ X \le x \} \right) \\
 \; &= \; P \left( \{ X \le x \} \right)
  - P \left( \{ X \le x \} \, \cap \, \{ X \le x - \epsilon \}  \right) \\
 \; &= \; P \left( \{ X \le x \} \right) 
  - P \left( \{ X \le x - \epsilon \}  \right) \\
 \; &= \; P \left( X \le x \right) - P \left( X \le x - \epsilon \right) \\
 \; &= \; F_X (x) - F_X(x - \epsilon)
\end{aligned}
$$

Therefore,
$$
0 \le P(X = x) \; \le \; \lim_{\epsilon \rightarrow 0+} F_X(x) - F_X(x - \epsilon) 
  \; = \; 0 .  \quad \blacksquare
$$


## Probability density function

### Definition

The probability density function (pdf) of a continuous random variable is 
a function $f_X(x)$ that satisfies
$$
F_X(x) = \int_{-\infty}^x f_X(t) \, dt \quad \text{ for all } x .
$$

Further, if $f_X(x)$ is continuous, by the Fundamental Theorem of Calculus,
$$
\frac{d}{dx} F_X(x) = f_X(x) .
$$

### Theorem

A function $f_X(x)$ is a pmf of a random variable $X$ if and only if
a. f_X(x) \ge 0 \text{for all} x.
b. \sum_x f_X(x) = 1.

A function $f_X(x)$ is a pdf of a random variable $X$ if and only if
a. f_X(x) \ge 0 \text{for all} x.
b. \int_{-\infty}^\infty f_X(x) dx = 1.


## Notation

If a random variable $X$ has a cdf $F_X(x)$, we write
$$
X \sim F_X(x) ,
$$
where the $\sim$ (tilde) symbol means "is distributed according to".

The right-hand-side of the $\sim$ operator can be anything that help defines
a probability distribution.

For instance, if $X$ has a pmf or pdf $f_X(x)$, we can also write
$$
X \sim f_X(x) .
$$

If randomv variables $X$ and $Y$ have the same distribution, we write
$$
X \sim Y .
$$


## Summary

"Math is not magic."  - High school math teacher

\bigskip

Casella & Berger 2002, sections 1.4--1.6 (pages 27--37).

Exercises (page 37) 1.1, 1.6, 1.44--1.48, 1.52--1.55.

\bigskip

### Intended learning outcomes {.c}

- Recognize and define events and random variables
- Implement code to sample values for a random variable

